# Анализ статьи "KAN: Kolmogorov-Arnold Networks"

- [Ссылка на статью](https://arxiv.org/abs/2404.19756)

### Общие свойства

- Функции активации на ребрах а не в узлах (нейронах)

- Интерпретируемость результатов

- Исследование "scaling laws" и проклятья размерности

- Применение энтропийной регуляризации

- Тестирование на специальных функциях и датасете с функциями из книг Фейнмана

- KAN дает альтернативные конструкции для сложных функций

### Описание сплайнов

- KAN-слой реализуется как B-сплайн с residual connections:
    $$
    \phi(x) = w(b(x) + spline(x))
    $$
    где $b(x) = silu(x) = \frac{x}{1 + e^{-x}}$

- Оптимизация такого сплайна нетрививальна, и для улучшения сходимости сплайн инициализирует так, чтобы быть близким к нулю в начальный момент времени. Cетка с узлами сплайна обновляется на лету.

- KAN обладает лучшей масштабируемостью в сравнении c MLP и обходит проклятие размерности за счет того, что представляет многомерную функцию в виде композиции одномерных, переводя задачу в низкоразмерное пространство.

### Примеры применения

- Рассмотривается релятивистскую скорость $f(u, v) = \frac{u+v}{1+uv}$. 
Конструкция довольно сложная, потому что предполагается, что умножение $u$ на $v$ будет использовать два слоя, обращение $(1 + uv)$ будет использовать один слой, и умножение $(u + v)$ и $(\frac{1}{1 + uv})$ будет использовать еще два слоя, в результате предполагается всего 5 слоев. Однако автоматически созданная архитектура KANs имеют всего 2 слоя. 

- В ретроспективе это ожидаемо, если вспомнить трюк с "быстротой" в теории относительности: определим две быстроты: 
    1. $\equiv \text{arctanh}\ u$ 
    2. $\equiv \text{arctanh}\ v$. 

- Релятивистское составление скоростей является простым сложением в пространстве быстрот, то есть:
$$
\frac{u+v}{1+uv} = \text{tanh}(\text{arctanh}\ u + \text{arctanh}\ v)
$$
- Это может быть реализовано двухслойным KAN. Представив, что мы не знаем понятия быстроты в физике, мы могли бы потенциально открыть это понятие прямо из KANs без проб и ошибок символических манипуляций.

- Рассмотрим уравнение Пуассона с нулевыми граничными условиями Дирихле. Для $\Omega = [-1, 1]^2$, рассмотрим уравнение в частных производных. Используем принципы физически обоснованных нейронных сетей (PINN) для решения этого УЧП, с функцией потерь, заданной как сумма: 
    1. $\text{loss}_i$ - внутренних потерь, дискретизированных и оцененных на равномерной сетке $n_i$ точек $z_i = (x_i, y_i)$ внутри расчетной области, 
    2. $\text{loss}_b$ - граничных потерь, дискретизированных и оцененных на равномерной сетке $n_b$ точек на границе.
- Функция потерь для PINNs:
$$
    \mathcal{L}(f ; \mathcal{T}) = \text{loss}_i + \alpha \text{loss}_b
    $$
- $\alpha$ является гиперпараметром, уравновешивающим влияние двух слагаемых. Сравнивается архитектура KAN с MLP, используя равные гиперпараметры $n_i = 10000$, $n_b = 800$, и $\alpha = 0.01$. Измеряется ошибка по норме $L_2$ и в энергетической $H^1$. В результате видим, что KAN достигает гораздо лучшего закона масштабирования с меньшей ошибкой, используя меньшие сети и меньшее количество параметров. 
- Поэтому предполагается, что KAN могут иметь потенциал для метода PINN.
