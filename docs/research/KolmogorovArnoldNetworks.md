# Анализ статьи "KAN: Kolmogorov-Arnold Networks"

- [Ссылка на статью](https://arxiv.org/abs/2404.19756)

### Общие свойства

- Функции активации на ребрах, а не в узлах (нейронах)

- Интерпретируемость результатов

- Исследование "scaling laws" и проклятья размерности

- Применение энтропийной регуляризации

- Тестирование на специальных функциях и датасете с функциями из книг Фейнмана

- KAN дает альтернативные конструкции для сложных функций (пример, спец. функция Бесселя)

## Теорема Колмогорова-Арнольда
Пусть есть $f$ многомерная непрерывная функция, тогда $f$ можно записать в виде конечной композиции непрерывных функций одной переменной и бинарной операции сложения, иными словами для любой непрерывной функции $f(x_1, \ldots, x_n)$ существуют такие непрерывные функции $\Phi_q $ и $\phi_{i,j}$, что:
$$
f(x_1, \ldots, x_n) = \sum_{q=0}^{2n} \Phi_q \left(\sum_{p=1}^{n} \phi_{q,p}(x_p)\right). 
$$
S
Это представление позволяет KAN использовать обучаемые функции активации, параметризованные как сплайны, на ребрах сети, что отличается от традиционных MLP, где функции активации фиксированы и применяются к узлам. В KAN каждый вес в сети заменяется на одномерную функцию.

## Архитектура KAN

Введем некоторые обозначения. Форма KAN представлена целочисленным массивом:
$$
[n_0, n_1, \ldots, n_L]
$$
где $n_i$​ - это количество узлов в $i$-ом слое вычислительного графа. Обозначаем $i$-ый нейрон в $l$-ом слое как $(l,i)$, а значение активации нейрона $(l,i)$ как $x_{l,i}$​. Между слоем $l$ и слоем $l+1$ есть $n_l ​n_{l+1}$​ функций активации: функция активации, которая соединяет $(l,i)$ и $(l+1,j)$, обозначается как
$$
\phi_{l,j,i}, \quad l = 0, \ldots, L - 1, \quad i = 1, \ldots, n_l, \quad j = 1, \ldots, n_{l+1}
$$
Преактивация $ϕ_{l,j,i​}$ просто $x_{l,i}$​; постактивация $ϕ_{l,j,i}$​ обозначается как $\widetilde{x}_{l,j,i} ​≡ ϕ_{l,j,i}​(x_{l,i}​)$. Значение активации нейрона $(l+1,j)$ просто сумма всех входящих постактиваций:
$$
x_{l+1,j} = \sum_{i=1}^{n_l} \tilde{x}_{l,j,i} = \sum_{i=1}^{n_l} \phi_{l,j,i}(x_{l,i}), \quad j = 1, \ldots, n_{l+1}
$$
В матричной форме это выглядит так:
$$
x_{l+1} = \begin{pmatrix}
\phi_{l,1,1}(\cdot) & \phi_{l,1,2}(\cdot) & \ldots & \phi_{l,1,n_l}(\cdot) \\
\phi_{l,2,1}(\cdot) & \phi_{l,2,2}(\cdot) & \ldots & \phi_{l,2,n_l}(\cdot) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_{l,n_{l+1},1}(\cdot) & \phi_{l,n_{l+1},2}(\cdot) & \ldots & \phi_{l,n_{l+1},n_l}(\cdot)
\end{pmatrix} \Bigg\} \Phi_l x_l
$$
где $Φ_l$​ - это функциональная матрица, соответствующая $l$-ому слою KAN. Общая сеть KAN - это композиция $L$ слоев: при заданном входном векторе $x_0​∈R^{n_0}$​, выход KAN является:
$$
KAN(x) = (\Phi_{L-1} \circ \Phi_{L-2} \circ \ldots \circ \Phi_{1} \circ \Phi_{0})x
$$
Можем переписать вышеуказанное уравнение, предполагая, что размерность выхода $n_L​=1$, и определить $f(x)≡KAN(x)$:
$$
f(x) = \sum_{i_{L-1}=1}^{n_{L-1}} \phi_{L-1,i_L,i_{L-1}} \left( \sum_{i_{L-2}=1}^{n_{L-2}} \ldots \sum_{i_2=1}^{n_2} \phi_{2,i_3,i_2} \sum_{i_1=1}^{n_1} \phi_{1,i_2,i_1} \sum_{i_0=1}^{n_0} \phi_{0,i_1,i_0} (x_{i_0}) \ldots \right)
$$
что довольно громоздко. Теорема Колмогорова-Арнольда в исходной формулировке соответствует 2-слойному KAN с формой $[n, 2n + 1, 1]$, то есть является частным случаем этой формулы. 
Все операции дифференцируемы, поэтому можно обучать KAN с помощью обратного распространения ошибки.

### Аппроксимационная теорема (перевод)

**Теория аппроксимации, KAT:**

Пусть $\mathbf{x} = (x_1, x_2, \ldots, x_n)$. Предположим, что функция $f(\mathbf{x})$ допускает представление:
$$
f = (\Phi_{L-1} \circ \Phi_{L-2} \circ \ldots \circ \Phi_{1} \circ \Phi_{0})\mathbf{x} 
$$
где каждая из функций $\Phi_{l,i,j}$ является  $(k + 1)$-раз непрерывно дифференцируемой. Тогда существует константа $C$ зависящая от $f$ и её представления, такая что мы имеем следующую границу аппроксимации в терминах размера сетки $G$: существуют функции B-сплайна $k$-го порядка $\Phi^G_{l,i,j}$ такие что для любого $0 \leq m \leq k$, мы имеем границу:
$$
\|f - (\Phi^G_{L-1} \circ \Phi^G_{L-2} \circ \ldots \circ \Phi^G_{1} \circ \Phi^G_{0})\mathbf{x}\|_{C^m} \leq C G^{-k-1+m}
$$
Здесь мы используем обозначение нормы $C^m$ измеряющей величину производных до порядка $m$:
$$
\|g\|_{C^m} = \max_{|\beta| \leq m} \sup_{\mathbf{x} \in [0,1]^n} \left| D^\beta g(\mathbf{x}) \right|.
$$

**Доказательство:**

По классической теории одномерных B-сплайнов и факту, что $\Phi_{l,i,j}$ как непрерывные функции могут быть равномерно ограничены на ограниченной области, мы знаем, что существуют функции B-сплайнов с конечной сеткой $\Phi^G_{l,i,j}$ такие, что для любого $0 \leq m \leq k$:
$$
\|(\Phi_{l,i,j} \circ \Phi_{l-1} \circ \ldots \circ \Phi_{0})\mathbf{x} - (\Phi^G_{l,i,j} \circ \Phi_{l-1} \circ \ldots \circ \Phi_{0})\mathbf{x}\|_{C^m} \leq C G^{-k-1+m} ,
$$
с константой $C$ не зависящей от $G$. Мы фиксируем эти аппроксимации B-сплайнов. Следовательно, мы имеем, что остаток $R_l$ определенный через:
$$
R_l := (\Phi^G_{L-1} \circ \ldots \circ \Phi^G_{l+1} \circ \Phi_{l} \circ \Phi_{l-1} \circ \ldots \circ \Phi_{0})\mathbf{x} - (\Phi^G_{L-1} \circ \ldots \circ \Phi^G_{l+1} \circ \Phi^G_{l} \circ \Phi_{l-1} \circ \ldots \circ \Phi_{0})\mathbf{x}
$$
удовлетворяет:
$$
\|R_l\|_{C^m} \leq C G^{-k-1+m} ,
$$
с константой, не зависящей от G. Наконец, заметим, что:
$$
f - (\Phi^G_{L-1} \circ \Phi^G_{L-2} \circ \ldots \circ \Phi^G_{1} \circ \Phi^G_{0})\mathbf{x} = R_{L-1} + R_{L-2} + \ldots + R_{1} + R_{0} ,
$$
мы знаем, что выполняется:
$$
\|f - (\Phi^G_{L-1} \circ \Phi^G_{L-2} \circ \ldots \circ \Phi^G_{1} \circ \Phi^G_{0})\mathbf{x}\|_{C^m} \leq C G^{-k-1+m}
$$

### Описание сплайнов

- KAN-слой реализуется как B-сплайн с residual connections:
    $$
    \phi(x) = w(b(x) + spline(x))
    $$
    где $b(x) = silu(x) = \frac{x}{1 + e^{-x}}$

- Оптимизация такого сплайна нетрививальна, и для улучшения сходимости сплайн инициализирует так, чтобы быть близким к нулю в начальный момент времени. Cетка с узлами сплайна обновляется на лету.

- KAN обладает лучшей масштабируемостью в сравнении c MLP и обходит проклятие размерности за счет того, что представляет многомерную функцию в виде композиции одномерных, переводя задачу в низкоразмерное пространство.

## Техники упрощения

### Разрежение 
Для MLP используется L1-регуляризация линейных весов, чтобы способствовать разреженности. KAN могут адаптировать эту высокоуровневую идею, но требуются две модификации:
- В KAN нет линейного “веса”. Линейные веса заменяются обучаемыми функциями активации, поэтому мы должны определить L1-норму этих функций активации.
- Было обнаружено, что L1 недостаточно для разрежения KAN; вместо этого необходима дополнительная регуляризация энтропии.

Мы определяем L1-норму функции активации ϕ как ее среднюю величину по ее $N_p$​ входам, т.е.
$$
|\phi|_1 \equiv \frac{1}{N_p} \sum_{s=1}^{N_p} |\phi(x^{(s)})|
$$
Затем для слоя KAN $Φ$ с $n_in$​ входами и $n_out​$ выходами мы определяем L1-норму $Φ$ как сумму L1-норм всех функций активации, т.е.
$$
|\Phi|_1 \equiv \sum_{i=1}^{n_{in}} \sum_{j=1}^{n_{out}} |\phi_{i,j}|_1
$$
Кроме того, мы определяем энтропию $Φ$ как 
$$
S(\Phi) \equiv -\sum_{i=1}^{n_{in}} \sum_{j=1}^{n_{out}} \frac{|\phi_{i,j}|_1}{|\Phi|_1} \log \left( \frac{|\phi_{i,j}|_1}{|\Phi|_1} \right)
$$
Общая цель обучения $ℓ_total$ является суммой потерей предсказания ℓ_pred​ и L1 и регуляризация энтропии всех слоев KAN:
$$
\ell_{total} = \ell_{pred} + \lambda \left( \mu_1 \sum_{l=0}^{L-1} |\Phi_l|_1 + \mu_2 \sum_{l=0}^{L-1} S(\Phi_l) \right)
$$
где $μ_1$​,$μ_2$​ - относительные величины, обычно устанавливаемые как $μ_1​=μ_2​=1$, и $λ$ контролирует общую величину регуляризации.

### Визуализация
Когда мы визуализируем KAN, чтобы получить представление о величинах, мы устанавливаем прозрачность функции активации $ϕ_{l,i,j}$​ пропорционально $tanh(β \cdot A_{l,i,j}​)$, где $β=3$.

### Обрезка
После обучения с штрафом за разрежение мы также можем захотеть обрезать сеть до меньшей подсети. Мы разрежаем KAN на уровне узлов (а не на уровне ребер). Для каждого узла (скажем, $i$-го нейрона в $l$-ом слое) мы определяем его входящий и исходящий баллы как:
$$
I_{l,i} = \max_k (|\phi_{l-1,k,i}|_1), \quad O_{l,i} = \max_j (|\phi_{l+1,j,i}|_1),
$$
и считаем узел важным, если оба входящих и исходящих балла больше порогового гиперпараметра $θ=10^{−2}$ по умолчанию. Все неважные нейроны удаляются.

### Символизация
В случаях, когда мы подозреваем, что некоторые функции активации на самом деле символические (например, cos или log), мы предоставляем интерфейс для установки их в определенную символическую форму,

## Примеры применения

1. Конструкция довольно сложная, потому что предполагается, что умножение $u$ на $v$ будет использовать два слоя, обращение $(1 + uv)$ будет использовать один слой, и умножение $(u + v)$ и $(\frac{1}{1 + uv})$ будет использовать еще два слоя, в результате предполагается всего 5 слоев. Однако автоматически созданная архитектура KANs имеют всего 2 слоя. 

    В ретроспективе это ожидаемо, если вспомнить трюк с "быстротой" в теории относительности: определим две быстроты: 
        1. $\equiv \text{arctanh}\ u$ 
        2. $\equiv \text{arctanh}\ v$. 

    Релятивистское составление скоростей является простым сложением в пространстве быстрот, то есть:
    $$
    \frac{u+v}{1+uv} = \text{tanh}(\text{arctanh}\ u + \text{arctanh}\ v)
    $$

    Это может быть реализовано двухслойным KAN. Представив, что мы не знаем понятия быстроты в физике, мы могли бы потенциально открыть это понятие прямо из KANs без проб и ошибок символических манипуляций.

2. Рассмотрим уравнение Пуассона с нулевыми граничными условиями Дирихле. Для $\Omega = [-1, 1]^2$, рассмотрим уравнение в частных производных. 
    $$
    u_{xx} + u_{yy} = f \text{ в } \Omega, \\
    u = 0 \text{ в } \partial\Omega.
    $$

    Пусть $f$:
    $$
    f = -\frac{\pi^2}{2}(1 + 4y^2) \sin(\pi x) \sin(\pi y^2) + 2\pi \sin(\pi x) \cos(\pi y^2)
    $$

    Истинное решение этой задачи:
    $$
    u = \sin(\pi x) \sin(\pi y^2)
    $$

    Используем принципы физически обоснованных нейронных сетей (PINN) для решения этого УЧП, с функцией потерь, заданной как сумма: 
        1. $\text{loss}_i$ - внутренних потерь, дискретизированных и оцененных на равномерной сетке $n_i$ точек $z_i = (x_i, y_i)$ внутри расчетной области, 
        2. $\text{loss}_b$ - граничных потерь, дискретизированных и оцененных на равномерной сетке $n_b$ точек на границе.

    Функция потерь для PINNs:
    $$
    \mathcal{L}(f ; \mathcal{T}) = \text{loss}_i + \alpha \text{loss}_b = 
    \frac{1}{n_b} \sum_{i=1}^{n_b} u^2 + \alpha \left( \frac{1}{n_i} \sum_{i=1}^{n_i} |u_{xx}(z_i) + u_{yy}(z_i) - f(z_i)|^2 \right),
    $$

    $\alpha$ является гиперпараметром, уравновешивающим влияние двух слагаемых. 

    Сравнивается архитектура KAN с MLP, используя равные гиперпараметры $n_i = 10000$, $n_b = 800$, и $\alpha = 0.01$. Измеряется ошибка по норме $L_2$ и в энергетической $H^1$. В результате видим, что KAN достигает гораздо лучшего закона масштабирования с меньшей ошибкой, используя меньшие сети и меньшее количество параметров. 
    Поэтому предполагается, что KAN могут иметь потенциал для метода PINN.
