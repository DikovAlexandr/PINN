# Анализ статьи "On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs"

- [Ссылка на статью](https://arxiv.org/pdf/2004.01806.pdf)

### Обозначение проблем
- Исследуются вопросы сходимости решения, получаемого с помощью PINN, к решению уравнения.
- Адаптируя подход Шаудера и принцип максимума, мы показываем, что последовательность минимизаторов сильно сходится к решению УЧП в C0. Кроме того, мы показываем, что, если каждый минимизатор удовлетворяет начальным/граничным условиям, режим сходимости становится H1. Приводятся вычислительные примеры, иллюстрирующие наши теоретические выводы. Насколько нам известно, это первая теоретическая работа, показывающая непротиворечивость PINN.
- Ссылка на теорему Пинкуса (A. Pinkus. Approximation theory of the mlp model in neural networks). В других статьях показали, что однослойная нейронная сеть с достаточно большой ширины может равномерно аппроксимировать функцию и ее частную производную. 
- Также было показано, что нейронные сети способны аппроксимировать решения для некоторых классов УЧП. Статья для квазилинейных параболических уравнений - J. Sirignano and K. Spiliopoulos. DGM: A deep learning algorithm for solving partial differential
equations. Journal of Computational Physics, 375:1339–1364, 2018.
- Для обучения обычно используются методы градиентной оптимизации. Было предложено множество вариантов метода стохастического градиентного спуска, Adam и L-BFGS широко используются для PINN. 
- Эмпирически показано, что решения, найденные с помощью градиентной оптимизации, хорошо справляются с различными сложными задачами. Однако, нет никакой гарантии, что оптимизация на основе градиента найдет глобальный минимум для задачи.
- Ошибка оценки (estimation) возникают из-за использования конечных данных и является одной из двух составляющих (вместе с ошибкой аппроксимации) ошибки обобщения (generalization). 
- В машинном обучении ошибка обобщения является мерой точности прогнозирования на неизвестных данных. В задачах УЧП ошибка обобщения — это расстояние между лучшим минимизатором функции потерь и решением УЧП.

### Вопросы сходимости
- В статье рассматриваются линейные эллиптические и параболические уравнения второго порядка, которые допускают классические решения (по-видимому, речь про решение в квадратурах). 
- Последовательность минимизаторов регуляризованных потерь сходится к решению УЧП равномерно. Другими словами, ошибка обобщения PINN сходится к нулю при однородной топологии. 
- В статье показывается, что если минимизаторы удовлетворяют начальным/краевым условиям, режим сходимости становится H1. Чтобы гарантировать существование, регулярность и единственность решения, применяется подход Шаудера. 
- Это первая теоретическая работа, которая доказывает математически работоспособность PINN.

Дальнейшие математические гипотизы и выводы требуют подробного анализа на бумаге.