### Общее:

Вопросы и пожелания:
1. Было бы здорово посмотреть на результаты в презентации, а не в ipynb. 
    - Сделано.
2. Непонятно как выбирались основные гиперпараметры типа функций активации, архитектуры и тп. вещей. Было ли проведено исследование?  
    - Результаты исследования есть в отчете и в презентации.
3. Почему не используются готовые библиотеки типа DeepXDE?
    - Было интересно сделать собственную реализацию. Некоторые вещи заимствованы из DeepXDE.
4. Какая цель использовать здесь PINN? Что ускоряем/улучшаем?
    - Исследование возможностей современных численных методов.

Что можно попробовать?
1. Взять не такую глубокую нейросеть, но с большим числом нейронов, если требуется. Может прозвучит странно, но у нас при решении уравнений глубокие архитектуры (особенно просто FNN) вели себя хуже. Если у них другой опыт, то хотелось бы на такую задачу посмотреть.
    - Лучше всего себя проявляют сети глубиной примерно 4 слоя. Количество нейронов в каждом слое должно быть достаточно большим, примерно 30-60.
2. Изучить другие функции активации. Это бесконечно важный элемент, у нас лучше работал sin (SIRENA). Tanh показывал результаты заметно хуже.
    - Tanh работает не сильно хуже. На данный момент, значимых преимуществ sin замечено не было. 

3. Исследовать влияние оптимизатора LBFGS. Мы сейчас в основном работаем с Adam/YOGI + нормальным scheduler'ом. Исключительно LBFGS давал нам результаты похуже + сходился бесконечно долго. Но ходит слушок, что Adam+LBFGS может тоже показать что-то интересное. Также можно посмотреть тактики с lookahead. Но ещё раз, из нашего опыта: просто правильный scheduler вместе с классическим Adam'ом вообще очень сильно меняет картину.
    - **Текущая задача**
    1. Разобраться с scheduler'ом.
    2. Как объединить Adam и LBFGS.
    3. Что дает lookahead и как использовать.

4. Как определяется количество точек в области/на границе? Следим ли мы за тем, чтобы random покрыл всю область? Можно использовать регулярные + рандомизированные сетки. Можно обновлять сетку каждые N итераций, если не хватает мощностей покрыть всю область сразу. Но надо изучить как именно сетка раскидана.
    - **Текущая задача**
    1. Добавить адаптивную генерацию точек
    2. Сделать возможность использования curriculum learning.
    3. Сделать вывод того, в точках какого класса ошибка больше.


5. Как настраивается регуляризация? Почему выбраны именно такие коэффициенты в функции потерь?
    - Сделать этот выбор настраиваемыми параметрами
6. Исследовалось ли где сидит наибольшая ошибка (не невязка)? (может исходя из этого накидывалась регуляризация?)
    - Не очень понятно о каком типе ошибки идет речь в таком случае. Регуляризацию лучше делать на основе значения соответствующей компоненты функции потерь.
7. Можно ли добавить относительную ошибку в рассмотрение?
    - Не знаком с термином относительная ошибка, пользуюсь терминологией из статей:
        1. Аппроксимация - разница между действительным решением и самой ближайшей из функций, которую можно выбрать из всего семейства функций, которые могут быть представлены выбранной сетью.
        2. Оценка - связана с конечностью набора точек для обучения.
        2. Генерализация - связанная с выбором точек (их положением и количеством), в которых находятся остатки. Ошибки аппроксимации и оценки вместе дают ошибку генерализации.
        4. Оптимизация - связана с тем, что оптимизатор ищет лучшую функцию из семейства, но находит лишь ее аппроксимацию, с точностью зависящей от learning rate и числа шагов.