# Анализ лекции "Designing Next-Generation Numerical Methods with Physics-Informed Neural Networks"

- [Ссылка на лекцию](https://www.youtube.com/watch?v=BiB82F_fgUw&ab_channel=NHR%40FAU)

### Идеи
1. Можно делать аппроксимацию на узлах сетки, значения в которых просчитаны классическим методом.
2. Для сходимости нужно следить как меняется ошибка и где она больше.
3. ReLU, ELU не подходящие функции активации.
4. Для PINN есть смысл использовать два оптимизатора одновременно. Первый Adam, второй - оптимизатор Бройдена-Флетчера-Голдфарба-Шенно (BFGS). BFGS это метод второго порядка, он использует матрицу Гессе (кривизну в многомерном пространстве). Однако без использования оптимизатора Adam возможна сходимость к локальному минимуму. По этой причине сначала используется оптимизатор Adam, чтобы избежать локальных минимумов, а затем решение уточняется с помощью BFGS. BFGS в настоящее время является наиболее передовой технологией для PINN, поскольку она обеспечивает наиболее более высокую точность.
5. Можно использовать transfer learning.
6. По точности, PINN все же сложно сравниться с традиционными итеративными методами. Однако можно использовать комбинацию методов.
7. Сначала сходимость PINN происходит на крупномасштабных структурах. 
Принцип частоты (F-принцип): Глубокие нейронные сети часто соответствуют целевым функциям от низких до высоких частот в процессе обучения.
F-принцип подразумевает, что в PINN сначала проявляются низкочастотные/крупномасштабные особенности решения, тогда как для восстановления высокочастотных/мелкомасштабных особенностей потребуется больше эпох обучения.
8. Если мы поменяем, к примеру, начальные условия, нам понадобится повторно тренировать сеть, однако мы можем использовать веса от предыдущей задачи как стартовую точку (transfer learning). Обобщающей способности в привычном смысле нет.

# Анализ лекции "Rethinking Physics Informed Neural Networks"

- [Ссылка на лекцию](https://www.youtube.com/watch?v=qYmkUXH7TCY&ab_channel=AmirGholaminejad)

### Идеи
1. Введение веса для остатков на точках уравнения.
2. Обучение "по учебной программе" (curriculum learning): начните с простых физических ограничений и постепенно вводите сложности во время обучения. Сначала позвольте нейронной сети изучить простые задачи и закономерности, прежде чем оптимизировать веса на изучении целевого PDE. Этот подход базируется на упрощении данных для первых эпох обучения.
Пример: первоначально тренируйте NN с небольшими скоростями в уравнении переноса и медленно увеличивайте скорость до целевого значения.
3. PINN легко реализовать, но с их обучением связано множество тонкостей.
PINNS может не справиться с простыми задачами, такими как задачи адвекции, реакции и/или реакции-диффузии с нетривиальными коэффициентами. [characterizing pinns failure modes]
4. Анализ проблемы показывает, что, хотя НС имеет достаточную мощность для изучения решения, проблему оптимизации с мягкой регуляризацией PINN становится очень сложно решить.
5. Последовательное обучение: вместо того, чтобы пытаться предсказать все пространство-время сразу, поставьте задачу как Seq2Seq и позвольте PINN научиться предсказывать определенные моменты времени.
6. В отличие от всех других классических задач ML, PINN нельзя оптимизировать с помощью mini-batch (SGD, ADAM и т. д. не работают). Единственный работающий метод — это LBFGS на полноразмерной выборке. Этот факт делает обучение PINN вычислительно дорогим. (утверждение под сомнением)
7. Классическая архитектура нейронной сети может быть неоптимальной для PINN. Необходимо изучить альтернативные архитектуры, которые больше подходят для непрерывного характера проблемы. (речь про функцию активации) Необходимо изучить, какую архитектуру следует применять для конкретной задачи. Эллиптические, гиперболические и параболические PDE могут нуждаться в разных архитектурах.

# Анализ лекции "PINNs for Solving Non-linear PDEs, Neural Stochastic Partial Differential Equations"

- [Ссылка на лекцию](https://www.youtube.com/watch?v=eqcXwc8RLCA&ab_channel=CRUNCHGroup%3AHomeofMath%2BMachineLearning%2BX)

### Идеи
1. Обычные реализации PINN с не могут дать качественное приближение к аналитическому решению для 1D гиперболического УЧП с наличием ударных волн.
2. Согласно универсальной теореме аппроксимации (Цыбенко, 1989) должна существовать сеть, которая может обеспечить близкую аппроксимацию непрерывного решения. Сложность заключается в том, как искать это решение, т. е. процесс оптимизации или выбор функция потерь. 
3. Решение проблемы может быть в добавлении диффузионного члена (регуляризации PDE): Limitations of Physics Informed Machine Learning for Nonlinear Two-phase Transport in Porous Media (2020)
4. Neural Machine translation by learning to align and translate - использование encoder/decoder архитектуры
5. Изначальные веса выбираются не случайным образом, а из нормального распределения с нулевым мат. ожиданием и дисперсией 1/N, где N - число входных нейронов. Этот подход называется инициализация Ксавье (Xavier initialization).