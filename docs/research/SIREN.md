# Анализ статьи "Implicit Neural Representations with Periodic Activation Functions"

- [Ссылка на статью](https://arxiv.org/abs/2006.09661)

### Общие слова
- Cовременные сети неспособны моделировать сигналы с высокой детализацией и не могут представлять пространственные и временные производные сигнала, неявно определяемых как решение уравнений в частных производных
- В статье есть примеры использования для изображений, волновых полей, видео, звука и их производных, решения сложных краевых задач уравнения Эйконала, уравнение Пуассона, а также уравнения Гельмгольца и волновые уравнения. (если сделать разделение перменных в волновом уравнении получится уравнение Геймгольца)

### Введение
1. Введено предположение о классе функций (Phi это функция нейронной сети)
2. Эта функция задает взимнооднозначное соответствие между (координатами) и (цветом) что позволяет знать цвет на субпиксельном уровне.
3. Периодические функции способны точнее передавать детали сигналов

### Основные выводы
1. Полносвязные нейронные дают эффективное по памяти представление объектов
2. Использование периодических функций активации проводилось и ранее. Вводились нейронные сети Фурье (имитация разложения в ряд Фурье однослойной сетью)
3. MLP с плавными, непериодическими функциями активации не могут точно моделировать высокочастотную информацию и производные более высокого порядка. 
4. В то время как неявные нейронные представления могут использоваться для непосредственного решения ОДУ или УЧП путем наблюдения за динамикой системы, нейронные ОДУ позволяют моделировать непрерывные функции путем объединения обычного решателя ОДУ (например, неявного алгоритма Адамса или Рунге-Кутты) с сетью, которая параметризует динамику функции.
5. Вводится оптимизационная проблема
6. Афинное преобразование - то, которое можно произвести матрицей перехода (оно и линейное смещение являются аргументами синуса активации)
7. Функция распределения весов должна быть конкретной, иначе будет низкая точность и скорость сходимости
8. Частоты для малых аргументов (менее Pi/4) не меняются, так как синус фактически линеен, для значений больших этого, частота увеличивается
9. Подобранные параметры хорошо работают для оптимизатора Adam
10. Так как производная синуса это косинус, производная от SIREN это просто смещенная сеть
11. С изображениями работают похожим на PINN образом - в функции потерь сравнивают производную по выводу сети с производной от изображения

Дальнейшие математические гипотизы и выводы требуют подробного анализа на бумаге.