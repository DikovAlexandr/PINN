# Анализ статьи "Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs"

- [Ссылка на статью](https://arxiv.org/pdf/2006.16144.pdf)

### Общие слова
- Уравнения в частных производных (ЧДЭ) моделируют широкий спектр природных и техногенных явлений во всех областях науки и техники. Явные формулы решения доступны только для узкого спектра постановок - отсюда следует распростроненное использование численного моделирования для большинства практических приложений. 
- Доступно множество разнообразных методов численной аппроксимации УЧП, таких как метод конечных разностей, метод конечных элементов, метод конечного объема и спектральные методы.
- Однако, численное моделирование таких проблем, как количественная оценка неопределенности (UQ), многомасштабные и мультифизические задачи, обратные задачи и задачи оптимизации с ограничениями, PDE в областях с очень сложной геометрией и PDE в больших размерностях, по-прежнему затруднено.

### Варианты использования
- Один из подходов основан на явных (или полунеявных) формулах представления, таких как формула Фейнмана-Каца (математическая формула, устанавливающая связь между дифференциальными уравнениями с частными производными (специального типа) и случайными процессами, связана с методом Монте-Карло) для параболических (и эллиптических) PDE. Этот подход представлен и проанализирован для различных эллиптических и параболических УЧП, а также линейных уравнений переноса.
- Другой подход заключается в улучшении существующих численных методов путем добавления модулей, основанных на глубоком обучении.
- Третий подход заключается в использовании глубоких нейронных сетей для изучения наблюдаемых или интересующих величин, ррешения основных УЧП на основе этих данных.
- Глубокие нейронные сети обладают свойством универсальной аппроксимации (например G. Cybenko. Approximations by superpositions of sigmoidal functions. Approximation theory and its applications), а именно любая непрерывная, и даже измеримая, функция может быть аппроксимирована DNN. Отдельно можно посмотреть описание необходимой архитектуры нейронной сети для функций с достаточной соболевской регулярностью (D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks).

### Историческая справка
- Использовать глубокие нейронные сети для решений УЧП, в частности путем вычисления остатка PDE в точках обучения (подробное описание см. в разделе 2, алгоритм 2.3 статьи IEEE).
- Этот подход спустя 20 лет был возрожден и разработан значительно более подробно благодаря новаторскому вкладу Карниадакиса и его сотрудников (первые статьи про PINN).
- Они ввели термин PINN и после их статей произошел рост публикаций, в которых представлены алгоритмы с PINN для различных
приложения.

### Вопросы сходимости
- В других статьях было предложено множество эвристических закономерностей того, как данные, режимы обучение и прочее влияют на сходимость нейронной сети к минимуму. Однако существует мало строгих обоснования того, почему PINN работают. За исключением статьи "On the convergence and generalization of physics informed
neural network".
- Основная цель — предоставить некоторое обоснование того, почему PINN настолько эффективны при аппроксимации решений прямой проблемы для PDE при разумной и поддающейся проверке гипотезе о лежащем в основе PDE. 
- Ключевой вопрос — понять механизмы, с помощью которых минимизация остатков PDE в точках коллокации, которая является основным компонентом алгоритма обучения PINN, может привести к контролю (ограничению) общей ошибки. 
- Для исследования этих вопросов представляется абстрактная структура PINN, которая охватывает широкий спектр потенциальных приложений, в том числе для нелинейных PDE, и доказываются оценки ошибки обобщения (generalization), то есть ошибки нейронной сети при прогнозировании на не показанных данее данных. 
- Абстрактная оценка ограничивает ошибку обобщения с точки зрения базовой ошибки обучения, количества точек обучения и границ устойчивости для основного уравнения. 
- Оценка ошибки обобщения покажет, что ошибка аппроксимации основного уравения с помощью обученного PINN будет достаточно низкой, пока выполняются условия:
    1. Ошибка обучения невелика, т. е. PINN хорошо обучен. Эта ошибка вычисляется и отслеживается в процессе обучения. Следовательно, оно доступно апостериорно. Речь про значение функции потерь.
    2. Количество точек обучения (коллокации) достаточно велико. Это число определяется ошибкой из-за основного правила квадратур (рассмотрено далее).
    3. Решение основного УЧП устойчиво с очень высокой точностью (по отношению к возмущениям входных данных). Для нелинейных УЧП эти границы устойчивости могут потребовать, чтобы решения основных УЧП (и PINN) были достаточно регулярными.
    4. Неявные константы, возникающие в оценках устойчивости и квадратурных ошибок, которые зависят от базовых PINN, необходимо контролировать подходящим образом.
- Таким образом, с помощью полученной оценки ошибки мы определяем возможные механизмы, с помощью которых PINN могут так хорошо аппроксимировать PDE, и обеспечиваем прочную математическую основу для аппроксимации PINN.
- Представлены три примера, чтобы проиллюстрировать абстрактную структуру и оценку погрешности
    1. Линейные и квазилинейные параболические уравнения, 
    2. Одномерные скалярные квазилинейные параболические (и гиперболические) законы сохранения
    3. Несжимаемое уравнение Эйлера гидродинамики. 
- Абстрактная оценка погрешности описана в конкретных терминах для каждого примера, а численные эксперименты представлены для подтверждения предложенной теории. Цель состоит в том, чтобы убедить читателя в том, почему PINN, если они правильно сформулированы, успешно численно аппроксимируют прямую задачу для PDE.

Дальнейшие математические гипотизы и выводы требуют подробного анализа на бумаге.