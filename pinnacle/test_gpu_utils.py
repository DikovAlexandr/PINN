"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö GPU —É—Ç–∏–ª–∏—Ç.

–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç—ã—Ö –∏ –ø–æ–ª–µ–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GPU.
"""

import sys
import io
import os

# –î–ª—è Windows: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ UTF-8 –≤ –∫–æ–Ω—Å–æ–ª–∏
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

os.environ["DDE_BACKEND"] = "pytorch"

import torch
import deepxde as dde


def test_device_selection():
    """–¢–µ—Å—Ç 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢ 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞")
    print("="*70)
    
    device = dde.utils.get_optimal_device(verbose=True)
    print(f"\n‚úì –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤—ã–±—Ä–∞–Ω–æ: {device}")
    
    assert device in [torch.device('cuda'), torch.device('cpu')], "–ù–µ–≤–µ—Ä–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"
    print("‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")


def test_memory_info():
    """–¢–µ—Å—Ç 2: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ GPU"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢ 2: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ GPU")
    print("="*70)
    
    info = dde.utils.get_gpu_memory_info()
    
    print(f"\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏:")
    for key, value in info.items():
        if key != 'error':
            print(f"  {key}: {value:.2f} GB" if isinstance(value, float) and key != 'utilization' else f"  {key}: {value:.1f}%" if key == 'utilization' else f"  {key}: {value}")
    
    assert 'allocated' in info, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç 'allocated' –≤ info"
    assert 'total' in info, "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç 'total' –≤ info"
    
    print("\n‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")


def test_cache_clearing():
    """–¢–µ—Å—Ç 3: –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢ 3: –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("\n‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç")
        return
    
    # –°–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ GPU
    device = torch.device('cuda')
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ GPU...")
    tensors = [torch.randn(1000, 1000, device=device) for _ in range(5)]
    
    info_before = dde.utils.get_gpu_memory_info()
    print(f"–ü–∞–º—è—Ç—å –¥–æ –æ—á–∏—Å—Ç–∫–∏: {info_before['reserved']:.2f} GB")
    
    # –£–¥–∞–ª–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã
    del tensors
    
    # –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à
    dde.utils.clear_gpu_cache(verbose=True)
    
    info_after = dde.utils.get_gpu_memory_info()
    print(f"–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {info_after['reserved']:.2f} GB")
    
    print("\n‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")


def test_memory_context():
    """–¢–µ—Å—Ç 4: Context manager –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢ 4: Context manager –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("\n‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç")
        return
    
    device = torch.device('cuda')
    
    print("\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ gpu_memory_context:")
    with dde.utils.gpu_memory_context(verbose=True):
        # –°–æ–∑–¥–∞—Ç—å –±–æ–ª—å—à–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã
        print("  –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤...")
        tensors = [torch.randn(5000, 1000, device=device) for _ in range(3)]
    
    print("\n‚úì –ü–∞–º—è—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∞")
    print("‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")


def test_tensor_memory_estimation():
    """–¢–µ—Å—Ç 5: –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–∞"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢ 5: –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–∞")
    print("="*70)
    
    test_cases = [
        ((10000, 100), torch.float32, "10k x 100 (float32)"),
        ((1000, 1000), torch.float64, "1k x 1k (float64)"),
        ((100, 100, 100), torch.float32, "100 x 100 x 100 (float32)"),
    ]
    
    print("\nüìè –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤:")
    for shape, dtype, description in test_cases:
        memory = dde.utils.estimate_tensor_memory(shape, dtype)
        print(f"  {description}: {memory:.4f} GB")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    memory_10k = dde.utils.estimate_tensor_memory((10000, 100), torch.float32)
    expected = (10000 * 100 * 4) / 1e9  # 4 bytes per float32
    assert abs(memory_10k - expected) < 1e-6, "–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø–∞–º—è—Ç–∏"
    
    print("\n‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")


def test_integration():
    """–¢–µ—Å—Ç 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–µ–π"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢ 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–µ–π")
    print("="*70)
    
    # –í—ã–±—Ä–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = dde.utils.get_optimal_device(verbose=False)
    print(f"\n–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    if device.type == 'cuda':
        info = dde.utils.get_gpu_memory_info()
        print(f"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å: {info['free']:.2f} GB")
    
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞
    print("\n–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏...")
    
    def pde(x, y):
        dy_xx = dde.grad.hessian(y, x, i=0, j=0)
        return dy_xx + 1
    
    geom = dde.geometry.Interval(0, 1)
    bc = dde.icbc.DirichletBC(geom, lambda x: 0, lambda _, on_boundary: on_boundary)
    
    data = dde.data.PDE(geom, pde, bc, num_domain=100, num_boundary=20)
    net = dde.nn.FNN([1, 20, 20, 1], "tanh", "Glorot uniform")
    model = dde.Model(data, net)
    
    model.compile("adam", lr=1e-3)
    
    # –û–±—É—á–µ–Ω–∏–µ —Å context manager
    with dde.utils.gpu_memory_context(clear_cache=True):
        model.train(iterations=100, display_every=100)
    
    print("\n‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")
    print("‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")


def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    print("\n" + "="*70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï GPU –£–¢–ò–õ–ò–¢")
    print("="*70)
    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GPU.")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    if torch.cuda.is_available():
        print(f"\n‚úì CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
        print(f"‚úì CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
    else:
        print("\n‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    try:
        test_device_selection()
        test_memory_info()
        test_cache_clearing()
        test_memory_context()
        test_tensor_memory_estimation()
        test_integration()
        
        print("\n" + "="*70)
        print("–í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´! ‚úì")
        print("="*70)
        print("\nGPU —É—Ç–∏–ª–∏—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
        print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:")
        print("  - dde.utils.get_optimal_device()     # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞")
        print("  - dde.utils.get_gpu_memory_info()    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏")
        print("  - dde.utils.clear_gpu_cache()        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞")
        print("  - dde.utils.gpu_memory_context()     # Context manager")
        print("  - dde.utils.estimate_tensor_memory() # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

